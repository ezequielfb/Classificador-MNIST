# üß† Classificador de D√≠gitos MNIST com TensorFlow/Keras

Projeto completo, desenvolvido no Google Colab, para construir, treinar e avaliar uma Rede Neural capaz de classificar d√≠gitos manuscritos (0 a 9) do famoso dataset MNIST.

## üìã Tabela de Conte√∫dos
* [Sobre o Projeto](##-sobre-o-projeto)
* [ Ferramentas Utilizadas](#Ô∏è-ferramentas-utilizadas)
* [ Como Executar o Projeto](#-como-executar-o-projeto)
* [ Resultados](#-resultados)

## Sobre o Projeto
O objetivo central √© demonstrar um fluxo de trabalho de ponta a ponta em Machine Learning: desde a prepara√ß√£o dos dados e constru√ß√£o do modelo, at√© o treinamento e a avalia√ß√£o de sua performance. O modelo √© uma Rede Neural simples, mas eficaz, que aprende a reconhecer padr√µes em imagens de 28x28 pixels para fazer suas classifica√ß√µes.

## Ferramentas Utilizadas
* **Ambiente de Desenvolvimento:** `Google Colab`
* **Biblioteca de Machine Learning:** `TensorFlow` com a API `Keras`
* **Bibliotecas de Apoio:** `NumPy` para manipula√ß√£o de dados e `Matplotlib` para visualiza√ß√£o.

## Como Executar o Projeto
Abra o notebook (`.ipynb`) no [Google Colab](https://colab.research.google.com/) e siga os passos abaixo, executando cada c√©lula de c√≥digo em sequ√™ncia.

### Passo 1: Ativar o Acelerador de Hardware (GPU)
Para um treinamento muito mais r√°pido, √© recomendado utilizar a GPU gratuita do Colab.
1. No menu, navegue at√© **Ambiente de execu√ß√£o > Alterar o tipo de ambiente de execu√ß√£o**.
2. Na janela que se abre, selecione **`T4 GPU`** no menu suspenso "Acelerador de hardware".
3. Clique em "Salvar".

### Passo 2: Importar as Bibliotecas
Vamos carregar todas as depend√™ncias necess√°rias para o projeto.
```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

print("Bibliotecas importadas com sucesso!")
```
Passo 3: Carregar o Dataset MNIST
O Keras facilita o acesso ao dataset MNIST, que j√° vem dividido em conjuntos de treino e teste.

```
# Carrega o dataset e j√° o divide em partes de treino e de teste
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

print(f"Temos {x_train.shape[0]} imagens para treino e {x_test.shape[0]} para teste.")
print(f"Cada imagem tem o tamanho de {x_train.shape[1]}x{x_train.shape[2]} pixels.")
```
Passo 4: Pr√©-processamento e Visualiza√ß√£o dos Dados
Normalizamos os valores dos pixels das imagens (de 0-255 para 0-1) para melhorar a performance do treinamento.
```
# Normalizando os valores dos pixels dividindo por 255
x_train = x_train / 255.0
x_test = x_test / 255.0
```
```
# esse c√≥digo voc√™ vai usar para dar uma olhada em uma imagem de exemplo para ver como ficou
plt.imshow(x_train[0], cmap='gray')
plt.title(f"Esta imagem representa o n√∫mero: {y_train[0]}")
plt.show()
```
Passo 5: Construir a Arquitetura da Rede Neural
Aqui, definimos a estrutura do nosso modelo, camada por camada.
```
model = keras.Sequential([
    # 1¬™ Camada (Entrada): Achata a imagem de 28x28 pixels em uma √∫nica linha de 784 pixels.
    keras.layers.Flatten(input_shape=(28, 28)),

    # 2¬™ Camada (Oculta): Uma camada "densa" com 128 neur√¥nios para aprender os padr√µes.
    # A fun√ß√£o de ativa√ß√£o 'relu' √© uma escolha padr√£o e muito eficiente.
    keras.layers.Dense(128, activation='relu'),

    # 3¬™ Camada (Regulariza√ß√£o): "Desliga" aleatoriamente 20% dos neur√¥nios durante o treino
    # para evitar que a rede apenas "decore" as imagens (overfitting).
    keras.layers.Dropout(0.2),

    # 4¬™ Camada (Sa√≠da): A camada final. Tem 10 neur√¥nios, um para cada d√≠gito (0 a 9).
    # A ativa√ß√£o 'softmax' transforma a sa√≠da em um conjunto de 10 probabilidades que somam 1.
    keras.layers.Dense(10, activation='softmax')
])

# Mostra um resumo da arquitetura que criamos
model.summary()
```
Passo 6: Compilar o Modelo
Configuramos os par√¢metros de aprendizado do modelo.
 * Otimizador (optimizer): adam √© o algoritmo que ajusta os pesos da rede para minimizar o erro.
 * Fun√ß√£o de Perda (loss): sparse_categorical_crossentropy calcula o qu√£o errada est√° a previs√£o do modelo.
 * M√©tricas (metrics): accuracy (precis√£o) √© o que monitoramos para avaliar a performance.
```
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```
Passo 7: Treinar o Modelo
Alimentamos o modelo com os dados de treino para que ele aprenda. epochs=5 significa que o modelo ver√° todo o conjunto de dados 5 vezes.
```
print("Iniciando o treinamento...")
history = model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))
print("Treinamento finalizado!")
```
## Resultados
Passo 8: Avaliar a Acur√°cia
Ap√≥s o treinamento, usamos o conjunto de teste (imagens que o modelo nunca viu) para verificar sua real capacidade de generaliza√ß√£o.
```
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)

print(f'\nAcur√°cia final no conjunto de teste: {test_acc * 100:.2f}%')

A acur√°cia final obtida no conjunto de teste foi de aproximadamente 97-98%.
```
Passo 9: Fazer uma Previs√£o
Vamos testar o modelo com uma imagem aleat√≥ria do conjunto de teste para ver um exemplo pr√°tico de sua previs√£o.
```
# Fazendo previs√µes para todo o conjunto de teste
predictions = model.predict(x_test)

# Escolha um n√∫mero de 0 a 9999 para testar uma imagem
index_da_imagem = 150 # Voc√™ pode mudar esse n√∫mero!

# Mostra a imagem que estamos testando
plt.imshow(x_test[index_da_imagem], cmap='gray')
plt.show()

# A sa√≠da do modelo √© um array de 10 probabilidades. O 'argmax' pega o √≠ndice com a maior probabilidade.
previsao_do_modelo = np.argmax(predictions[index_da_imagem])
resposta_correta = y_test[index_da_imagem]

print(f"O modelo previu que este n√∫mero √©: {previsao_do_modelo}")
print(f"A resposta correta √©: {resposta_correta}")

if previsao_do_modelo == resposta_correta:
    print("O modelo acertou!")
else:
    print("O modelo errou.")
```

Projeto desenvolvido como parte dos meus estudos em Engenharia de Software e Intelig√™ncia Artificial.

